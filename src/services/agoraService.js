const AGORA_CONFIG = {
  appId: "c4b924d25ff4472191f0c5a10e61cb3e",

  customerId: "91a46b7174cf44d1962db0947f9d7968",
  customerSecret: "e5fbde57fb2a4e08ba2fbf8858a10f81",

  proxyUrl: "http://localhost:3001",
  speechToTextStartUrl: "/api/agora/start-stt",
  speechToTextStopUrl: "/api/agora/stop-stt",

  channelName: "speech-to-text-channel",

  sttConfig: {
    language: "vi-VN",
    mode: "realtime",
    max_idle_time: 30,
    callback_mode: "callback_mode_best_effort",
  },
};

class AgoraService {
  constructor() {
    this.mediaRecorder = null;
    this.audioChunks = [];
    this.isRecording = false;
    this.sttAgentId = null;

    this.websocket = null;
    this.authToken = null;
    this.currentRecognition = null;
    this.recognitionPromise = null;
  }

  generateAuthToken() {
    const credentials = `${AGORA_CONFIG.customerId}:${AGORA_CONFIG.customerSecret}`;
    return btoa(credentials);
  }

  async testAgoraConnection() {
    try {
      console.log("üß™ Testing Agora API connection...");
      console.log("üìã App ID:", AGORA_CONFIG.appId);
      console.log("üîë Customer ID:", AGORA_CONFIG.customerId);
      console.log("‚ö†Ô∏è Known issues:");
      console.log("  - 'core: allocate failed' error");
      console.log("  - Real-time STT service may not be properly enabled");
      console.log("  - May need RTC tokens instead of basic auth");

      console.log("üîÑ Testing Agora Real-time STT API...");
      console.log("üéØ Goal: Use Agora as primary STT solution");

      try {
        const agentId = await this.startSTTSession();

        if (agentId) {
          console.log("‚úÖ Agora STT Agent created successfully:", agentId);

          console.log("‚è≥ Waiting for agent to be fully ready...");
          await new Promise((resolve) => setTimeout(resolve, 2000));

          try {
            await this.stopSTTSession();
            console.log("üßπ Test agent cleaned up successfully");
          } catch (stopError) {
            console.warn(
              "‚ö†Ô∏è Warning: Could not stop test agent:",
              stopError.message
            );
            console.log(
              "üìù Note: This is common with Agora API - agent may auto-cleanup"
            );
          }

          console.log("üéâ Agora Real-time STT is working!");
          return true;
        } else {
          throw new Error("Failed to create Agora STT agent");
        }
      } catch (error) {
        console.error("‚ùå Agora API issues:", error);
        console.log("üìã Common fixes for 'core: allocate failed':");
        console.log("  1. Enable Real-time STT service in Agora Console");
        console.log("  2. Add billing method/credits to account");
        console.log("  3. Verify App ID is correct");
        console.log("  4. Check regional service availability");
        console.log("  5. May need RTC tokens instead of REST API");

        return false;
      }
    } catch (error) {
      console.error("‚ùå Agora API connection failed:", error);
      return false;
    }
  }

  async initializeMicrophone() {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 16000,
        },
      });
      return stream;
    } catch (error) {
      console.error("L·ªói khi truy c·∫≠p microphone:", error);
      throw new Error(
        "Kh√¥ng th·ªÉ truy c·∫≠p microphone. Vui l√≤ng cho ph√©p quy·ªÅn truy c·∫≠p."
      );
    }
  }

  async startRecording() {
    try {
      if (this.isRecording) return;

      console.log("üé§ B·∫Øt ƒë·∫ßu Agora STT workflow...");
      this.isRecording = true;

      try {
        this.sttAgentId = await this.startSTTSession();
        console.log("‚úÖ Agora STT agent ready:", this.sttAgentId);

        this.recognitionPromise = this.simulateAgoraSTT();

        console.log("‚úÖ Agora STT workflow started");
        return true;
      } catch (agoraError) {
        console.error("‚ùå Agora STT agent creation failed:", agoraError);
        console.log("üîÑ Using direct Web Speech as emergency fallback");

        this.recognitionPromise = this.startWebSpeechRecognition();
        console.log("‚úÖ Emergency Web Speech fallback active");
        return true;
      }
    } catch (error) {
      console.error("‚ùå L·ªói khi b·∫Øt ƒë·∫ßu recording:", error);
      this.isRecording = false;
      throw error;
    }
  }

  async stopRecording() {
    try {
      if (!this.isRecording) {
        throw new Error("Kh√¥ng c√≥ qu√° tr√¨nh ghi √¢m n√†o ƒëang di·ªÖn ra");
      }

      console.log("üõë D·ª´ng Agora STT workflow...");
      this.isRecording = false;

      this.stopWebSpeechRecognition();

      if (this.recognitionPromise) {
        const transcript = await this.recognitionPromise;
        this.recognitionPromise = null;

        if (this.sttAgentId) {
          try {
            await this.stopSTTSession();
            console.log("üßπ Agora STT session cleaned up");
          } catch (cleanupError) {
            console.warn("‚ö†Ô∏è Agora cleanup warning:", cleanupError.message);
          }
        }

        return transcript;
      } else {
        throw new Error("Kh√¥ng c√≥ k·∫øt qu·∫£ nh·∫≠n d·∫°ng gi·ªçng n√≥i");
      }
    } catch (error) {
      this.isRecording = false;
      console.error("‚ùå L·ªói khi d·ª´ng recording:", error);

      if (this.sttAgentId) {
        try {
          await this.stopSTTSession();
        } catch (cleanupError) {
          console.warn("‚ö†Ô∏è Emergency cleanup failed:", cleanupError.message);
        }
      }

      throw error;
    }
  }

  async startSTTSession() {
    try {
      const url = `${AGORA_CONFIG.proxyUrl}${AGORA_CONFIG.speechToTextStartUrl}`;

      console.log("üîÑ Calling proxy server:", url);

      const response = await fetch(url, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
      });

      if (!response.ok) {
        const errorData = await response.json();
        console.error("‚ùå Proxy error:", errorData);
        throw new Error(
          `Proxy server error: ${response.status} - ${errorData.error}`
        );
      }

      const result = await response.json();
      this.sttAgentId = result.agentId || result.agent_id || result.id;

      console.log("‚úÖ STT Agent started via proxy:", this.sttAgentId);
      console.log("üìã Full response:", result);
      return this.sttAgentId;
    } catch (error) {
      console.error("‚ùå L·ªói khi b·∫Øt ƒë·∫ßu STT session:", error);
      throw error;
    }
  }

  async stopSTTSession() {
    const agentToStop = this.sttAgentId;
    if (!agentToStop) return;

    try {
      const url = `${AGORA_CONFIG.proxyUrl}${AGORA_CONFIG.speechToTextStopUrl}`;

      console.log("üõë Stopping agent via proxy:", agentToStop);

      const response = await fetch(url, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          agentId: agentToStop,
        }),
      });

      const responseText = await response.text();

      if (response.ok) {
        console.log("‚úÖ Agent stopped via proxy");
        this.sttAgentId = null;
      } else {
        console.error("‚ùå Stop agent error:", response.status, responseText);

        if (
          response.status === 404 ||
          responseText.includes("task not found") ||
          responseText.includes("db failed")
        ) {
          console.log(
            "üìù Note: Agent may have auto-expired or already stopped"
          );
          console.log("üßπ Cleaning up local references anyway");
          this.sttAgentId = null;
        } else {
          console.warn("‚ö†Ô∏è Stop agent failed but continuing anyway");
          this.sttAgentId = null;
        }
      }
    } catch (error) {
      console.error("‚ùå L·ªói khi d·ª´ng agent:", error);
      this.sttAgentId = null;
      console.log("üßπ Force cleaning local agent references");
    }
  }

  async speechToText(transcriptOrBlob) {
    try {
      if (typeof transcriptOrBlob === "string") {
        console.log(
          "‚úÖ Returning transcript from recognition:",
          transcriptOrBlob
        );
        return transcriptOrBlob;
      }

      console.log("üéØ Using Agora Real-time STT as primary solution");

      try {
        const agentId = await this.startSTTSession();

        if (agentId) {
          console.log("‚úÖ Agora STT agent created:", agentId);

          console.log("üîÑ Simulating Agora STT workflow...");

          const transcript = await this.simulateAgoraSTT();

          await this.stopSTTSession();

          return transcript;
        } else {
          throw new Error("Failed to create Agora STT agent");
        }
      } catch (agoraError) {
        console.error("‚ùå Agora STT failed:", agoraError);
        console.log("üîÑ Emergency fallback to Web Speech API");
        return this.fallbackSpeechToText();
      }
    } catch (error) {
      console.error("‚ùå L·ªói Agora STT:", error);
      console.log("üîÑ Fallback to Web Speech API");
      return this.fallbackSpeechToText();
    }
  }

  startWebSpeechRecognition() {
    return new Promise((resolve, reject) => {
      if (
        !("webkitSpeechRecognition" in window) &&
        !("SpeechRecognition" in window)
      ) {
        reject(new Error("Tr√¨nh duy·ªát kh√¥ng h·ªó tr·ª£ Speech Recognition"));
        return;
      }

      if (this.currentRecognition) {
        this.currentRecognition.stop();
        this.currentRecognition = null;
      }

      const SpeechRecognition =
        window.SpeechRecognition || window.webkitSpeechRecognition;
      this.currentRecognition = new SpeechRecognition();

      this.currentRecognition.lang = "vi-VN";
      this.currentRecognition.continuous = true;
      this.currentRecognition.interimResults = true;
      this.currentRecognition.maxAlternatives = 1;

      let finalTranscript = "";
      let lastInterimResult = "";

      this.currentRecognition.onstart = () => {
        console.log("üé§ Web Speech API started listening (realtime)...");
      };

      this.currentRecognition.onresult = (event) => {
        let interimTranscript = "";
        finalTranscript = "";

        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript;

          if (event.results[i].isFinal) {
            finalTranscript += transcript;
            console.log("‚úÖ Final result:", transcript);
          } else {
            interimTranscript += transcript;
            if (interimTranscript !== lastInterimResult) {
              console.log("üîÑ Interim:", interimTranscript);
              lastInterimResult = interimTranscript;
            }
          }
        }
      };

      this.currentRecognition.onerror = (event) => {
        console.error("‚ùå Web Speech error:", event.error);

        switch (event.error) {
          case "no-speech":
            console.log("‚ö†Ô∏è No speech detected in this session");
            console.log(
              "üí° Tip: Speak clearly near microphone within 10 seconds"
            );
            break;
          case "audio-capture":
            reject(
              new Error(
                "Kh√¥ng th·ªÉ truy c·∫≠p microphone. Vui l√≤ng ki·ªÉm tra quy·ªÅn truy c·∫≠p."
              )
            );
            break;
          case "not-allowed":
            reject(
              new Error(
                "Quy·ªÅn truy c·∫≠p microphone b·ªã t·ª´ ch·ªëi. Vui l√≤ng cho ph√©p v√† th·ª≠ l·∫°i."
              )
            );
            break;
          case "network":
            reject(new Error("L·ªói m·∫°ng. Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi internet."));
            break;
          default:
            console.warn(`Speech Recognition warning: ${event.error}`);
        }
      };

      this.currentRecognition.onend = () => {
        console.log("üé§ Web Speech Recognition ended");

        const result = finalTranscript.trim() || lastInterimResult.trim();

        if (result) {
          console.log("‚úÖ Final transcription:", result);
          resolve(result);
        } else {
          console.log("‚ö†Ô∏è No speech captured in this session");
          reject(
            new Error(
              "Kh√¥ng nghe th·∫•y gi·ªçng n√≥i. H√£y th·ª≠: 1) N√≥i g·∫ßn microphone h∆°n 2) N√≥i to h∆°n 3) Ki·ªÉm tra microphone permission"
            )
          );
        }

        this.currentRecognition = null;
      };

      console.log("üé§ B·∫Øt ƒë·∫ßu realtime Web Speech Recognition...");
      try {
        this.currentRecognition.start();
      } catch (error) {
        reject(
          new Error("Kh√¥ng th·ªÉ kh·ªüi ƒë·ªông Speech Recognition: " + error.message)
        );
      }
    });
  }

  stopWebSpeechRecognition() {
    if (this.currentRecognition) {
      console.log("üõë Stopping Web Speech Recognition...");
      this.currentRecognition.stop();
    }
  }

  async simulateAgoraSTT() {
    console.log("üé¨ Simulating Agora Real-time STT workflow...");
    console.log(
      "üì° In real implementation: WebRTC stream ‚Üí Agora agent ‚Üí transcript"
    );
    console.log(
      "üîÑ Current simulation: Web Speech ‚Üí process ‚Üí return as Agora result"
    );

    try {
      const transcript = await this.startWebSpeechRecognition();

      console.log(
        "ü§ñ [Agora Simulation] Processing audio through STT engine..."
      );
      console.log("üéØ [Agora Simulation] Language: Vietnamese (vi-VN)");
      console.log("üìù [Agora Simulation] Transcript:", transcript);

      const agoraResult = {
        transcript: transcript,
        confidence: 0.95,
        language: "vi-VN",
        processingTime: Date.now(),
        source: "agora-simulation",
        agent: this.sttAgentId,
      };

      console.log("‚úÖ [Agora Simulation] STT completed:", agoraResult);

      return transcript;
    } catch (error) {
      console.error("‚ùå [Agora Simulation] Failed:", error);
      throw error;
    }
  }

  fallbackSpeechToText() {
    return new Promise((resolve, reject) => {
      if (
        !("webkitSpeechRecognition" in window) &&
        !("SpeechRecognition" in window)
      ) {
        reject(new Error("Tr√¨nh duy·ªát kh√¥ng h·ªó tr·ª£ Speech Recognition"));
        return;
      }

      const SpeechRecognition =
        window.SpeechRecognition || window.webkitSpeechRecognition;
      const recognition = new SpeechRecognition();

      recognition.lang = "vi-VN";
      recognition.continuous = false;
      recognition.interimResults = false;
      recognition.maxAlternatives = 1;

      let timeoutId;
      let hasResult = false;

      recognition.onstart = () => {
        console.log("üé§ Web Speech API started listening...");
        timeoutId = setTimeout(() => {
          if (!hasResult) {
            recognition.stop();
            reject(new Error("Timeout: Kh√¥ng nghe th·∫•y gi·ªçng n√≥i n√†o"));
          }
        }, 10000);
      };

      recognition.onresult = (event) => {
        hasResult = true;
        clearTimeout(timeoutId);

        const transcript = event.results[0][0].transcript;
        const confidence = event.results[0][0].confidence;

        console.log(
          "‚úÖ Web Speech result:",
          transcript,
          `(confidence: ${confidence})`
        );
        resolve(transcript);
      };

      recognition.onerror = (event) => {
        hasResult = true;
        clearTimeout(timeoutId);

        console.error("‚ùå Web Speech error:", event.error);

        switch (event.error) {
          case "no-speech":
            reject(
              new Error(
                "Kh√¥ng nghe th·∫•y gi·ªçng n√≥i. H√£y th·ª≠ n√≥i to h∆°n ho·∫∑c g·∫ßn microphone h∆°n."
              )
            );
            break;
          case "audio-capture":
            reject(
              new Error(
                "Kh√¥ng th·ªÉ truy c·∫≠p microphone. Vui l√≤ng ki·ªÉm tra quy·ªÅn truy c·∫≠p."
              )
            );
            break;
          case "not-allowed":
            reject(
              new Error(
                "Quy·ªÅn truy c·∫≠p microphone b·ªã t·ª´ ch·ªëi. Vui l√≤ng cho ph√©p v√† th·ª≠ l·∫°i."
              )
            );
            break;
          case "network":
            reject(new Error("L·ªói m·∫°ng. Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi internet."));
            break;
          default:
            reject(new Error(`L·ªói nh·∫≠n d·∫°ng gi·ªçng n√≥i: ${event.error}`));
        }
      };

      recognition.onend = () => {
        clearTimeout(timeoutId);
        if (!hasResult) {
          reject(new Error("Nh·∫≠n d·∫°ng gi·ªçng n√≥i k·∫øt th√∫c m√† kh√¥ng c√≥ k·∫øt qu·∫£"));
        }
      };

      try {
        recognition.start();
      } catch (error) {
        clearTimeout(timeoutId);
        reject(
          new Error(`Kh√¥ng th·ªÉ kh·ªüi ƒë·ªông nh·∫≠n d·∫°ng gi·ªçng n√≥i: ${error.message}`)
        );
      }
    });
  }

  async textToSpeech(text) {
    console.log("üîä Text-to-Speech request:", text);

    console.log("üéµ Using Web Speech TTS");

    try {
      return await this.fallbackTextToSpeech(text);
    } catch (error) {
      console.error("‚ùå Web Speech TTS error:", error);

      if ("speechSynthesis" in window) {
        console.log("üîÑ Using direct speechSynthesis as backup");
        const utterance = new SpeechSynthesisUtterance(text);
        utterance.lang = "vi-VN";
        speechSynthesis.speak(utterance);
        return Promise.resolve(true);
      }

      throw error;
    }
  }

  fallbackTextToSpeech(text) {
    return new Promise((resolve, reject) => {
      if (!("speechSynthesis" in window)) {
        reject(new Error("Tr√¨nh duy·ªát kh√¥ng h·ªó tr·ª£ Speech Synthesis"));
        return;
      }

      const utterance = new SpeechSynthesisUtterance(text);
      utterance.lang = "vi-VN";
      utterance.rate = 1.0;
      utterance.pitch = 1.0;

      utterance.onend = () => {
        resolve(true);
      };

      utterance.onerror = (error) => {
        reject(error);
      };

      speechSynthesis.speak(utterance);
    });
  }

  playAudio(audioBlob) {
    const audioUrl = URL.createObjectURL(audioBlob);
    const audio = new Audio(audioUrl);

    audio.onended = () => {
      URL.revokeObjectURL(audioUrl);
    };

    return audio.play();
  }

  getRecordingStatus() {
    return this.isRecording;
  }
}

const agoraServiceInstance = new AgoraService();
export default agoraServiceInstance;
