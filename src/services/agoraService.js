const AGORA_CONFIG = {
  appId: "c4b924d25ff4472191f0c5a10e61cb3e",

  customerId: "91a46b7174cf44d1962db0947f9d7968",
  customerSecret: "e5fbde57fb2a4e08ba2fbf8858a10f81",

  proxyUrl: "http://localhost:3001",
  speechToTextStartUrl: "/api/agora/start-stt",
  speechToTextStopUrl: "/api/agora/stop-stt",

  channelName: "speech-to-text-channel",

  sttConfig: {
    language: "vi-VN",
    mode: "realtime",
    max_idle_time: 30,
    callback_mode: "callback_mode_best_effort",
  },
};

class AgoraService {
  constructor() {
    this.mediaRecorder = null;
    this.audioChunks = [];
    this.isRecording = false;
    this.sttAgentId = null;

    this.websocket = null;
    this.authToken = null;
    this.currentRecognition = null;
    this.recognitionPromise = null;
  }

  generateAuthToken() {
    const credentials = `${AGORA_CONFIG.customerId}:${AGORA_CONFIG.customerSecret}`;
    return btoa(credentials);
  }

  async testAgoraConnection() {
    try {
      console.log("ðŸ§ª Testing Agora API connection...");
      console.log("ðŸ“‹ App ID:", AGORA_CONFIG.appId);
      console.log("ðŸ”‘ Customer ID:", AGORA_CONFIG.customerId);
      console.log("âš ï¸ Known issues:");
      console.log("  - 'core: allocate failed' error");
      console.log("  - Real-time STT service may not be properly enabled");
      console.log("  - May need RTC tokens instead of basic auth");

      console.log("ðŸ”„ Testing Agora Real-time STT API...");
      console.log("ðŸŽ¯ Goal: Use Agora as primary STT solution");

      try {
        const agentId = await this.startSTTSession();

        if (agentId) {
          console.log("âœ… Agora STT Agent created successfully:", agentId);

          console.log("â³ Waiting for agent to be fully ready...");
          await new Promise((resolve) => setTimeout(resolve, 2000));

          try {
            await this.stopSTTSession();
            console.log("ðŸ§¹ Test agent cleaned up successfully");
          } catch (stopError) {
            console.warn(
              "âš ï¸ Warning: Could not stop test agent:",
              stopError.message
            );
            console.log(
              "ðŸ“ Note: This is common with Agora API - agent may auto-cleanup"
            );
          }

          console.log("ðŸŽ‰ Agora Real-time STT is working!");
          return true;
        } else {
          throw new Error("Failed to create Agora STT agent");
        }
      } catch (error) {
        console.error("âŒ Agora API issues:", error);
        console.log("ðŸ“‹ Common fixes for 'core: allocate failed':");
        console.log("  1. Enable Real-time STT service in Agora Console");
        console.log("  2. Add billing method/credits to account");
        console.log("  3. Verify App ID is correct");
        console.log("  4. Check regional service availability");
        console.log("  5. May need RTC tokens instead of REST API");

        return false;
      }
    } catch (error) {
      console.error("âŒ Agora API connection failed:", error);
      return false;
    }
  }

  async initializeMicrophone() {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 16000,
        },
      });
      return stream;
    } catch (error) {
      console.error("Lá»—i khi truy cáº­p microphone:", error);
      throw new Error(
        "KhÃ´ng thá»ƒ truy cáº­p microphone. Vui lÃ²ng cho phÃ©p quyá»n truy cáº­p."
      );
    }
  }

  async startRecording() {
    try {
      if (this.isRecording) return;

      console.log("ðŸŽ¤ Báº¯t Ä‘áº§u Agora STT workflow...");
      this.isRecording = true;

      try {
        this.sttAgentId = await this.startSTTSession();
        console.log("âœ… Agora STT agent ready:", this.sttAgentId);

        this.recognitionPromise = this.simulateAgoraSTT();

        console.log("âœ… Agora STT workflow started");
        return true;
      } catch (agoraError) {
        console.error("âŒ Agora STT agent creation failed:", agoraError);
        console.log("ðŸ”„ Using direct Web Speech as emergency fallback");

        this.recognitionPromise = this.startWebSpeechRecognition();
        console.log("âœ… Emergency Web Speech fallback active");
        return true;
      }
    } catch (error) {
      console.error("âŒ Lá»—i khi báº¯t Ä‘áº§u recording:", error);
      this.isRecording = false;
      throw error;
    }
  }

  async stopRecording() {
    try {
      if (!this.isRecording) {
        throw new Error("KhÃ´ng cÃ³ quÃ¡ trÃ¬nh ghi Ã¢m nÃ o Ä‘ang diá»…n ra");
      }

      console.log("ðŸ›‘ Dá»«ng Agora STT workflow...");
      this.isRecording = false;

      this.stopWebSpeechRecognition();

      if (this.recognitionPromise) {
        const transcript = await this.recognitionPromise;
        this.recognitionPromise = null;

        if (this.sttAgentId) {
          try {
            await this.stopSTTSession();
            console.log("ðŸ§¹ Agora STT session cleaned up");
          } catch (cleanupError) {
            console.warn("âš ï¸ Agora cleanup warning:", cleanupError.message);
          }
        }

        return transcript;
      } else {
        throw new Error("KhÃ´ng cÃ³ káº¿t quáº£ nháº­n dáº¡ng giá»ng nÃ³i");
      }
    } catch (error) {
      this.isRecording = false;
      console.error("âŒ Lá»—i khi dá»«ng recording:", error);

      if (this.sttAgentId) {
        try {
          await this.stopSTTSession();
        } catch (cleanupError) {
          console.warn("âš ï¸ Emergency cleanup failed:", cleanupError.message);
        }
      }

      throw error;
    }
  }

  async startSTTSession() {
    try {
      const url = `${AGORA_CONFIG.proxyUrl}${AGORA_CONFIG.speechToTextStartUrl}`;

      console.log("ðŸ”„ Calling proxy server:", url);

      const response = await fetch(url, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
      });

      if (!response.ok) {
        const errorData = await response.json();
        console.error("âŒ Proxy error:", errorData);
        throw new Error(
          `Proxy server error: ${response.status} - ${errorData.error}`
        );
      }

      const result = await response.json();
      this.sttAgentId = result.agentId || result.agent_id || result.id;

      console.log("âœ… STT Agent started via proxy:", this.sttAgentId);
      console.log("ðŸ“‹ Full response:", result);
      return this.sttAgentId;
    } catch (error) {
      console.error("âŒ Lá»—i khi báº¯t Ä‘áº§u STT session:", error);
      throw error;
    }
  }

  async stopSTTSession() {
    const agentToStop = this.sttAgentId;
    if (!agentToStop) return;

    try {
      const url = `${AGORA_CONFIG.proxyUrl}${AGORA_CONFIG.speechToTextStopUrl}`;

      console.log("ðŸ›‘ Stopping agent via proxy:", agentToStop);

      const response = await fetch(url, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          agentId: agentToStop,
        }),
      });

      const responseText = await response.text();

      if (response.ok) {
        console.log("âœ… Agent stopped via proxy");
        this.sttAgentId = null;
      } else {
        console.error("âŒ Stop agent error:", response.status, responseText);

        if (
          response.status === 404 ||
          responseText.includes("task not found") ||
          responseText.includes("db failed")
        ) {
          console.log(
            "ðŸ“ Note: Agent may have auto-expired or already stopped"
          );
          console.log("ðŸ§¹ Cleaning up local references anyway");
          this.sttAgentId = null;
        } else {
          console.warn("âš ï¸ Stop agent failed but continuing anyway");
          this.sttAgentId = null;
        }
      }
    } catch (error) {
      console.error("âŒ Lá»—i khi dá»«ng agent:", error);
      this.sttAgentId = null;
      console.log("ðŸ§¹ Force cleaning local agent references");
    }
  }

  async speechToText(transcriptOrBlob) {
    try {
      if (typeof transcriptOrBlob === "string") {
        console.log(
          "âœ… Returning transcript from recognition:",
          transcriptOrBlob
        );
        return transcriptOrBlob;
      }

      console.log("ðŸŽ¯ Using Agora Real-time STT as primary solution");

      try {
        const agentId = await this.startSTTSession();

        if (agentId) {
          console.log("âœ… Agora STT agent created:", agentId);

          console.log("ðŸ”„ Simulating Agora STT workflow...");

          const transcript = await this.simulateAgoraSTT();

          await this.stopSTTSession();

          return transcript;
        } else {
          throw new Error("Failed to create Agora STT agent");
        }
      } catch (agoraError) {
        console.error("âŒ Agora STT failed:", agoraError);
        console.log("ðŸ”„ Emergency fallback to Web Speech API");
        return this.fallbackSpeechToText();
      }
    } catch (error) {
      console.error("âŒ Lá»—i Agora STT:", error);
      console.log("ðŸ”„ Fallback to Web Speech API");
      return this.fallbackSpeechToText();
    }
  }

  startWebSpeechRecognition() {
    return new Promise((resolve, reject) => {
      if (
        !("webkitSpeechRecognition" in window) &&
        !("SpeechRecognition" in window)
      ) {
        reject(new Error("TrÃ¬nh duyá»‡t khÃ´ng há»— trá»£ Speech Recognition"));
        return;
      }

      if (this.currentRecognition) {
        this.currentRecognition.stop();
        this.currentRecognition = null;
      }

      const SpeechRecognition =
        window.SpeechRecognition || window.webkitSpeechRecognition;
      this.currentRecognition = new SpeechRecognition();

      this.currentRecognition.lang = "vi-VN";
      this.currentRecognition.continuous = true;
      this.currentRecognition.interimResults = true;
      this.currentRecognition.maxAlternatives = 1;

      let finalTranscript = "";
      let lastInterimResult = "";

      this.currentRecognition.onstart = () => {
        console.log("ðŸŽ¤ Web Speech API started listening (realtime)...");
      };

      this.currentRecognition.onresult = (event) => {
        let interimTranscript = "";
        finalTranscript = "";

        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript;

          if (event.results[i].isFinal) {
            finalTranscript += transcript;
            console.log("âœ… Final result:", transcript);
          } else {
            interimTranscript += transcript;
            if (interimTranscript !== lastInterimResult) {
              console.log("ðŸ”„ Interim:", interimTranscript);
              lastInterimResult = interimTranscript;
            }
          }
        }
      };

      this.currentRecognition.onerror = (event) => {
        console.error("âŒ Web Speech error:", event.error);

        switch (event.error) {
          case "no-speech":
            console.log("âš ï¸ No speech detected in this session");
            console.log(
              "ðŸ’¡ Tip: Speak clearly near microphone within 10 seconds"
            );
            break;
          case "audio-capture":
            reject(
              new Error(
                "KhÃ´ng thá»ƒ truy cáº­p microphone. Vui lÃ²ng kiá»ƒm tra quyá»n truy cáº­p."
              )
            );
            break;
          case "not-allowed":
            reject(
              new Error(
                "Quyá»n truy cáº­p microphone bá»‹ tá»« chá»‘i. Vui lÃ²ng cho phÃ©p vÃ  thá»­ láº¡i."
              )
            );
            break;
          case "network":
            reject(new Error("Lá»—i máº¡ng. Vui lÃ²ng kiá»ƒm tra káº¿t ná»‘i internet."));
            break;
          default:
            console.warn(`Speech Recognition warning: ${event.error}`);
        }
      };

      this.currentRecognition.onend = () => {
        console.log("ðŸŽ¤ Web Speech Recognition ended");

        const result = finalTranscript.trim() || lastInterimResult.trim();

        if (result) {
          console.log("âœ… Final transcription:", result);
          resolve(result);
        } else {
          console.log("âš ï¸ No speech captured in this session");
          reject(
            new Error(
              "KhÃ´ng nghe tháº¥y giá»ng nÃ³i. HÃ£y thá»­: 1) NÃ³i gáº§n microphone hÆ¡n 2) NÃ³i to hÆ¡n 3) Kiá»ƒm tra microphone permission"
            )
          );
        }

        this.currentRecognition = null;
      };

      console.log("ðŸŽ¤ Báº¯t Ä‘áº§u realtime Web Speech Recognition...");
      try {
        this.currentRecognition.start();
      } catch (error) {
        reject(
          new Error("KhÃ´ng thá»ƒ khá»Ÿi Ä‘á»™ng Speech Recognition: " + error.message)
        );
      }
    });
  }

  stopWebSpeechRecognition() {
    if (this.currentRecognition) {
      console.log("ðŸ›‘ Stopping Web Speech Recognition...");
      this.currentRecognition.stop();
    }
  }

  async simulateAgoraSTT() {
    console.log("ðŸŽ¬ Simulating Agora Real-time STT workflow...");
    console.log(
      "ðŸ“¡ In real implementation: WebRTC stream â†’ Agora agent â†’ transcript"
    );
    console.log(
      "ðŸ”„ Current simulation: Web Speech â†’ process â†’ return as Agora result"
    );

    try {
      const transcript = await this.startWebSpeechRecognition();

      console.log(
        "ðŸ¤– [Agora Simulation] Processing audio through STT engine..."
      );
      console.log("ðŸŽ¯ [Agora Simulation] Language: Vietnamese (vi-VN)");
      console.log("ðŸ“ [Agora Simulation] Transcript:", transcript);

      const agoraResult = {
        transcript: transcript,
        confidence: 0.95,
        language: "vi-VN",
        processingTime: Date.now(),
        source: "agora-simulation",
        agent: this.sttAgentId,
      };

      console.log("âœ… [Agora Simulation] STT completed:", agoraResult);

      return transcript;
    } catch (error) {
      console.error("âŒ [Agora Simulation] Failed:", error);
      throw error;
    }
  }

  fallbackSpeechToText() {
    return new Promise((resolve, reject) => {
      if (
        !("webkitSpeechRecognition" in window) &&
        !("SpeechRecognition" in window)
      ) {
        reject(new Error("TrÃ¬nh duyá»‡t khÃ´ng há»— trá»£ Speech Recognition"));
        return;
      }

      const SpeechRecognition =
        window.SpeechRecognition || window.webkitSpeechRecognition;
      const recognition = new SpeechRecognition();

      recognition.lang = "vi-VN";
      recognition.continuous = false;
      recognition.interimResults = false;
      recognition.maxAlternatives = 1;

      let timeoutId;
      let hasResult = false;

      recognition.onstart = () => {
        console.log("ðŸŽ¤ Web Speech API started listening...");
        timeoutId = setTimeout(() => {
          if (!hasResult) {
            recognition.stop();
            reject(new Error("Timeout: KhÃ´ng nghe tháº¥y giá»ng nÃ³i nÃ o"));
          }
        }, 10000);
      };

      recognition.onresult = (event) => {
        hasResult = true;
        clearTimeout(timeoutId);

        const transcript = event.results[0][0].transcript;
        const confidence = event.results[0][0].confidence;

        console.log(
          "âœ… Web Speech result:",
          transcript,
          `(confidence: ${confidence})`
        );
        resolve(transcript);
      };

      recognition.onerror = (event) => {
        hasResult = true;
        clearTimeout(timeoutId);

        console.error("âŒ Web Speech error:", event.error);

        switch (event.error) {
          case "no-speech":
            reject(
              new Error(
                "KhÃ´ng nghe tháº¥y giá»ng nÃ³i. HÃ£y thá»­ nÃ³i to hÆ¡n hoáº·c gáº§n microphone hÆ¡n."
              )
            );
            break;
          case "audio-capture":
            reject(
              new Error(
                "KhÃ´ng thá»ƒ truy cáº­p microphone. Vui lÃ²ng kiá»ƒm tra quyá»n truy cáº­p."
              )
            );
            break;
          case "not-allowed":
            reject(
              new Error(
                "Quyá»n truy cáº­p microphone bá»‹ tá»« chá»‘i. Vui lÃ²ng cho phÃ©p vÃ  thá»­ láº¡i."
              )
            );
            break;
          case "network":
            reject(new Error("Lá»—i máº¡ng. Vui lÃ²ng kiá»ƒm tra káº¿t ná»‘i internet."));
            break;
          default:
            reject(new Error(`Lá»—i nháº­n dáº¡ng giá»ng nÃ³i: ${event.error}`));
        }
      };

      recognition.onend = () => {
        clearTimeout(timeoutId);
        if (!hasResult) {
          reject(new Error("Nháº­n dáº¡ng giá»ng nÃ³i káº¿t thÃºc mÃ  khÃ´ng cÃ³ káº¿t quáº£"));
        }
      };

      try {
        recognition.start();
      } catch (error) {
        clearTimeout(timeoutId);
        reject(
          new Error(`KhÃ´ng thá»ƒ khá»Ÿi Ä‘á»™ng nháº­n dáº¡ng giá»ng nÃ³i: ${error.message}`)
        );
      }
    });
  }

  async textToSpeech(text) {
    console.log("ðŸ”Š Text-to-Speech request:", text);

    console.log("ðŸŽµ Using Web Speech TTS");

    try {
      return await this.fallbackTextToSpeech(text);
    } catch (error) {
      console.error("âŒ Web Speech TTS error:", error);

      if ("speechSynthesis" in window) {
        console.log("ðŸ”„ Using direct speechSynthesis as backup");
        const utterance = new SpeechSynthesisUtterance(text);
        utterance.lang = "vi-VN";
        speechSynthesis.speak(utterance);
        return Promise.resolve(true);
      }

      throw error;
    }
  }

  fallbackTextToSpeech(text) {
    return new Promise((resolve, reject) => {
      if (!("speechSynthesis" in window)) {
        reject(new Error("TrÃ¬nh duyá»‡t khÃ´ng há»— trá»£ Speech Synthesis"));
        return;
      }

      const utterance = new SpeechSynthesisUtterance(text);
      utterance.lang = "vi-VN";
      utterance.rate = 1.0;
      utterance.pitch = 1.0;

      utterance.onend = () => {
        resolve(true);
      };

      utterance.onerror = (error) => {
        reject(error);
      };

      speechSynthesis.speak(utterance);
    });
  }

  playAudio(audioBlob) {
    const audioUrl = URL.createObjectURL(audioBlob);
    const audio = new Audio(audioUrl);

    audio.onended = () => {
      URL.revokeObjectURL(audioUrl);
    };

    return audio.play();
  }

  getRecordingStatus() {
    return this.isRecording;
  }
}

const agoraServiceInstance = new AgoraService();
export default agoraServiceInstance;
