// Agora Real-time Speech-to-Text Configuration
// D·ª±a tr√™n t√†i li·ªáu ch√≠nh th·ª©c: https://docs.agora.io/en/real-time-stt/get-started/quickstart
const AGORA_CONFIG = {
  // App ID t·ª´ Agora Console
  appId: "c4b924d25ff4472191f0c5a10e61cb3e",

  // Customer ID v√† Customer Secret t·ª´ RESTful API credentials
  // L∆ØU √ù: Trong production, kh√¥ng n√™n ƒë·ªÉ th√¥ng tin n√†y ·ªü client-side
  customerId: "91a46b7174cf44d1962db0947f9d7968", // L·∫•y t·ª´ Developer Toolkit > RESTful API
  customerSecret: "e5fbde57fb2a4e08ba2fbf8858a10f81", // L·∫•y t·ª´ Developer Toolkit > RESTful API

  // Proxy server endpoints (ƒë·ªÉ tr√°nh CORS)
  proxyUrl: "http://localhost:3001",
  speechToTextStartUrl: "/api/agora/start-stt",
  speechToTextStopUrl: "/api/agora/stop-stt",

  // Channel configuration cho STT
  channelName: "speech-to-text-channel",

  // STT Configuration
  sttConfig: {
    language: "vi-VN", // Ti·∫øng Vi·ªát
    mode: "realtime", // Real-time mode
    max_idle_time: 30, // Th·ªùi gian t·ªëi ƒëa kh√¥ng c√≥ audio (gi√¢y)
    callback_mode: "callback_mode_best_effort",
  },
};

class AgoraService {
  constructor() {
    this.mediaRecorder = null;
    this.audioChunks = [];
    this.isRecording = false;
    this.sttAgentId = null; // Changed from sttSessionId to sttAgentId
    this.websocket = null;
    this.authToken = null;
    this.currentRecognition = null; // For Web Speech API
    this.recognitionPromise = null; // Track current recognition promise
  }

  // T·∫°o Basic Authentication token cho REST API
  generateAuthToken() {
    const credentials = `${AGORA_CONFIG.customerId}:${AGORA_CONFIG.customerSecret}`;
    return btoa(credentials);
  }

  // Test Agora API connection
  async testAgoraConnection() {
    try {
      console.log("üß™ Testing Agora API connection...");
      console.log("üìã App ID:", AGORA_CONFIG.appId);
      console.log("üîë Customer ID:", AGORA_CONFIG.customerId);
      console.log("‚ö†Ô∏è Known issues:");
      console.log("  - 'core: allocate failed' error");
      console.log("  - Real-time STT service may not be properly enabled");
      console.log("  - May need RTC tokens instead of basic auth");

      // Skip Agora API test to avoid console errors
      console.log(
        "‚ö†Ô∏è Skipping Agora API test - known issues with 'core: allocate failed'"
      );
      console.log("üéØ Focus: Web Speech API is working perfectly for STT");
      console.log("üîä Text-to-Speech also working via Web Speech API");
      console.log("‚úÖ App is fully functional without Agora dependency");

      return false; // Agora not working, but app still fully functional

      /* TODO: Re-enable when Agora issues are resolved
      const sessionId = await this.startSTTSession();
      
      if (sessionId) {
        console.log("‚úÖ Agora API connection successful!");
        await this.stopSTTSession();
        return true;
      }
      
      return false;
      */
    } catch (error) {
      console.error("‚ùå Agora API connection failed:", error);
      return false;
    }
  }

  // Kh·ªüi t·∫°o microphone ƒë·ªÉ ghi √¢m
  async initializeMicrophone() {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 16000,
        },
      });
      return stream;
    } catch (error) {
      console.error("L·ªói khi truy c·∫≠p microphone:", error);
      throw new Error(
        "Kh√¥ng th·ªÉ truy c·∫≠p microphone. Vui l√≤ng cho ph√©p quy·ªÅn truy c·∫≠p."
      );
    }
  }

  // B·∫Øt ƒë·∫ßu ghi √¢m v·ªõi Web Speech API realtime
  async startRecording() {
    try {
      if (this.isRecording) return;

      console.log("üé§ B·∫Øt ƒë·∫ßu Web Speech Recognition realtime...");
      this.isRecording = true;

      // Start Web Speech API immediately for realtime recognition
      this.recognitionPromise = this.startWebSpeechRecognition();

      console.log("‚úÖ ƒê√£ b·∫Øt ƒë·∫ßu nh·∫≠n d·∫°ng gi·ªçng n√≥i realtime");
      return true;
    } catch (error) {
      console.error("‚ùå L·ªói khi b·∫Øt ƒë·∫ßu nh·∫≠n d·∫°ng gi·ªçng n√≥i:", error);
      this.isRecording = false;
      throw error;
    }
  }

  // D·ª´ng ghi √¢m v√† tr·∫£ v·ªÅ transcription
  async stopRecording() {
    try {
      if (!this.isRecording) {
        throw new Error("Kh√¥ng c√≥ qu√° tr√¨nh ghi √¢m n√†o ƒëang di·ªÖn ra");
      }

      console.log("üõë D·ª´ng Web Speech Recognition...");
      this.isRecording = false;

      // Stop current recognition
      this.stopWebSpeechRecognition();

      // Wait for recognition result
      if (this.recognitionPromise) {
        const transcript = await this.recognitionPromise;
        this.recognitionPromise = null;
        return transcript;
      } else {
        throw new Error("Kh√¥ng c√≥ k·∫øt qu·∫£ nh·∫≠n d·∫°ng gi·ªçng n√≥i");
      }
    } catch (error) {
      this.isRecording = false;
      console.error("‚ùå L·ªói khi d·ª´ng recording:", error);
      throw error;
    }
  }

  // B·∫Øt ƒë·∫ßu STT session v·ªõi Agora qua proxy server
  async startSTTSession() {
    try {
      const url = `${AGORA_CONFIG.proxyUrl}${AGORA_CONFIG.speechToTextStartUrl}`;

      console.log("üîÑ Calling proxy server:", url);

      const response = await fetch(url, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        // Proxy server s·∫Ω handle authentication v√† request body
      });

      if (!response.ok) {
        const errorData = await response.json();
        console.error("‚ùå Proxy error:", errorData);
        throw new Error(
          `Proxy server error: ${response.status} - ${errorData.error}`
        );
      }

      const result = await response.json();
      this.sttAgentId = result.agentId || result.agent_id || result.id;

      console.log("‚úÖ STT Agent started via proxy:", this.sttAgentId);
      console.log("üìã Full response:", result);
      return this.sttAgentId;
    } catch (error) {
      console.error("‚ùå L·ªói khi b·∫Øt ƒë·∫ßu STT session:", error);
      throw error;
    }
  }

  // D·ª´ng STT agent qua proxy
  async stopSTTSession() {
    if (!this.sttAgentId) return;

    try {
      const url = `${AGORA_CONFIG.proxyUrl}${AGORA_CONFIG.speechToTextStopUrl}`;

      console.log("üõë Stopping agent via proxy:", this.sttAgentId);

      const response = await fetch(url, {
        method: "POST", // Changed to POST theo Agora API
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          agentId: this.sttAgentId,
        }),
      });

      if (response.ok) {
        console.log("‚úÖ STT Agent stopped via proxy");
        this.sttAgentId = null;
      } else {
        const errorData = await response.json();
        console.error("‚ùå Stop agent error:", errorData);
      }
    } catch (error) {
      console.error("‚ùå L·ªói khi d·ª´ng STT agent:", error);
    }
  }

  // Chuy·ªÉn ƒë·ªïi Speech to Text - Web Speech API handled in startRecording/stopRecording
  async speechToText(transcriptOrBlob) {
    try {
      // If we receive a string (transcript), just return it
      if (typeof transcriptOrBlob === "string") {
        console.log(
          "‚úÖ Returning transcript from realtime recognition:",
          transcriptOrBlob
        );
        return transcriptOrBlob;
      }

      // Fallback to old method if needed (shouldn't happen with new flow)
      console.log("‚ö†Ô∏è Fallback: Processing audio blob with new recognition");
      return this.fallbackSpeechToText();

      /* TODO: Fix Agora integration sau khi resolve c√°c issues:
       * 1. "core: allocate failed, please check if the appid is valid"
       * 2. Real-time STT service ch∆∞a enable properly
       * 3. C√≥ th·ªÉ c·∫ßn RTC Token thay v√¨ Basic auth
       
      console.log("üîÑ Th·ª≠ g·ªçi Agora STT API...");

      // Test Agora API v·ªõi session management
      const sessionId = await this.startSTTSession();

      if (sessionId) {
        console.log("‚úÖ Agora STT Session created:", sessionId);

        // TODO: Implement actual audio streaming v·ªõi WebRTC
        // Hi·ªán t·∫°i ch·ªâ test API endpoints

        // D·ª´ng session sau khi test
        await this.stopSTTSession();

        // Fallback to Web Speech API cho actual transcription
        console.log("üìù S·ª≠ d·ª•ng Web Speech API cho transcription");
        return this.fallbackSpeechToText();
      } else {
        throw new Error("Kh√¥ng th·ªÉ t·∫°o Agora STT session");
      }
      */
    } catch (error) {
      console.error("‚ùå L·ªói Agora STT:", error);
      console.log("üîÑ Fallback to Web Speech API");
      return this.fallbackSpeechToText();
    }
  }

  // Web Speech API cho realtime recognition
  startWebSpeechRecognition() {
    return new Promise((resolve, reject) => {
      if (
        !("webkitSpeechRecognition" in window) &&
        !("SpeechRecognition" in window)
      ) {
        reject(new Error("Tr√¨nh duy·ªát kh√¥ng h·ªó tr·ª£ Speech Recognition"));
        return;
      }

      // Stop existing recognition if any
      if (this.currentRecognition) {
        this.currentRecognition.stop();
        this.currentRecognition = null;
      }

      const SpeechRecognition =
        window.SpeechRecognition || window.webkitSpeechRecognition;
      this.currentRecognition = new SpeechRecognition();

      // C·∫•u h√¨nh optimal cho realtime recognition
      this.currentRecognition.lang = "vi-VN";
      this.currentRecognition.continuous = true; // Continuous listening
      this.currentRecognition.interimResults = true; // Show interim results
      this.currentRecognition.maxAlternatives = 1;

      let finalTranscript = "";
      let lastInterimResult = "";

      this.currentRecognition.onstart = () => {
        console.log("üé§ Web Speech API started listening (realtime)...");
      };

      this.currentRecognition.onresult = (event) => {
        let interimTranscript = "";
        finalTranscript = "";

        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript;

          if (event.results[i].isFinal) {
            finalTranscript += transcript;
            console.log("‚úÖ Final result:", transcript);
          } else {
            interimTranscript += transcript;
            if (interimTranscript !== lastInterimResult) {
              console.log("üîÑ Interim:", interimTranscript);
              lastInterimResult = interimTranscript;
            }
          }
        }
      };

      this.currentRecognition.onerror = (event) => {
        console.error("‚ùå Web Speech error:", event.error);

        switch (event.error) {
          case "no-speech":
            console.log("‚ö†Ô∏è No speech detected, continuing to listen...");
            break;
          case "audio-capture":
            reject(
              new Error(
                "Kh√¥ng th·ªÉ truy c·∫≠p microphone. Vui l√≤ng ki·ªÉm tra quy·ªÅn truy c·∫≠p."
              )
            );
            break;
          case "not-allowed":
            reject(
              new Error(
                "Quy·ªÅn truy c·∫≠p microphone b·ªã t·ª´ ch·ªëi. Vui l√≤ng cho ph√©p v√† th·ª≠ l·∫°i."
              )
            );
            break;
          case "network":
            reject(new Error("L·ªói m·∫°ng. Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi internet."));
            break;
          default:
            console.warn(`Speech Recognition warning: ${event.error}`);
        }
      };

      this.currentRecognition.onend = () => {
        console.log("üé§ Web Speech Recognition ended");

        // Get the best available result
        const result = finalTranscript.trim() || lastInterimResult.trim();

        if (result) {
          console.log("‚úÖ Final transcription:", result);
          resolve(result);
        } else {
          reject(
            new Error("Kh√¥ng nghe th·∫•y gi·ªçng n√≥i n√†o. H√£y th·ª≠ n√≥i to h∆°n.")
          );
        }

        this.currentRecognition = null;
      };

      // B·∫Øt ƒë·∫ßu recognition
      console.log("üé§ B·∫Øt ƒë·∫ßu realtime Web Speech Recognition...");
      try {
        this.currentRecognition.start();
      } catch (error) {
        reject(
          new Error("Kh√¥ng th·ªÉ kh·ªüi ƒë·ªông Speech Recognition: " + error.message)
        );
      }
    });
  }

  // Stop current recognition
  stopWebSpeechRecognition() {
    if (this.currentRecognition) {
      console.log("üõë Stopping Web Speech Recognition...");
      this.currentRecognition.stop();
    }
  }

  // Fallback Speech-to-Text s·ª≠ d·ª•ng Web Speech API (old method)
  fallbackSpeechToText() {
    return new Promise((resolve, reject) => {
      if (
        !("webkitSpeechRecognition" in window) &&
        !("SpeechRecognition" in window)
      ) {
        reject(new Error("Tr√¨nh duy·ªát kh√¥ng h·ªó tr·ª£ Speech Recognition"));
        return;
      }

      const SpeechRecognition =
        window.SpeechRecognition || window.webkitSpeechRecognition;
      const recognition = new SpeechRecognition();

      // C·∫•u h√¨nh t·ªëi ∆∞u cho ti·∫øng Vi·ªát
      recognition.lang = "vi-VN";
      recognition.continuous = false;
      recognition.interimResults = false;
      recognition.maxAlternatives = 1;

      // Timeout v√† error handling c·∫£i thi·ªán
      let timeoutId;
      let hasResult = false;

      recognition.onstart = () => {
        console.log("üé§ Web Speech API started listening...");
        // Set timeout n·∫øu kh√¥ng c√≥ k·∫øt qu·∫£ sau 10 gi√¢y
        timeoutId = setTimeout(() => {
          if (!hasResult) {
            recognition.stop();
            reject(new Error("Timeout: Kh√¥ng nghe th·∫•y gi·ªçng n√≥i n√†o"));
          }
        }, 10000);
      };

      recognition.onresult = (event) => {
        hasResult = true;
        clearTimeout(timeoutId);

        const transcript = event.results[0][0].transcript;
        const confidence = event.results[0][0].confidence;

        console.log(
          "‚úÖ Web Speech result:",
          transcript,
          `(confidence: ${confidence})`
        );
        resolve(transcript);
      };

      recognition.onerror = (event) => {
        hasResult = true;
        clearTimeout(timeoutId);

        console.error("‚ùå Web Speech error:", event.error);

        // X·ª≠ l√Ω c√°c lo·∫°i l·ªói kh√°c nhau
        switch (event.error) {
          case "no-speech":
            reject(
              new Error(
                "Kh√¥ng nghe th·∫•y gi·ªçng n√≥i. H√£y th·ª≠ n√≥i to h∆°n ho·∫∑c g·∫ßn microphone h∆°n."
              )
            );
            break;
          case "audio-capture":
            reject(
              new Error(
                "Kh√¥ng th·ªÉ truy c·∫≠p microphone. Vui l√≤ng ki·ªÉm tra quy·ªÅn truy c·∫≠p."
              )
            );
            break;
          case "not-allowed":
            reject(
              new Error(
                "Quy·ªÅn truy c·∫≠p microphone b·ªã t·ª´ ch·ªëi. Vui l√≤ng cho ph√©p v√† th·ª≠ l·∫°i."
              )
            );
            break;
          case "network":
            reject(new Error("L·ªói m·∫°ng. Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi internet."));
            break;
          default:
            reject(new Error(`L·ªói nh·∫≠n d·∫°ng gi·ªçng n√≥i: ${event.error}`));
        }
      };

      recognition.onend = () => {
        clearTimeout(timeoutId);
        if (!hasResult) {
          reject(new Error("Nh·∫≠n d·∫°ng gi·ªçng n√≥i k·∫øt th√∫c m√† kh√¥ng c√≥ k·∫øt qu·∫£"));
        }
      };

      try {
        recognition.start();
      } catch (error) {
        clearTimeout(timeoutId);
        reject(
          new Error(`Kh√¥ng th·ªÉ kh·ªüi ƒë·ªông nh·∫≠n d·∫°ng gi·ªçng n√≥i: ${error.message}`)
        );
      }
    });
  }

  // Text-to-Speech s·ª≠ d·ª•ng Web Speech API
  // L∆∞u √Ω: Agora ch·ªß y·∫øu cung c·∫•p STT, TTS c√≥ th·ªÉ s·ª≠ d·ª•ng Web Speech API ho·∫∑c d·ªãch v·ª• kh√°c
  async textToSpeech(text) {
    try {
      // S·ª≠ d·ª•ng Web Speech API cho TTS
      return this.fallbackTextToSpeech(text);
    } catch (error) {
      console.error("L·ªói Text-to-Speech:", error);
      throw error;
    }
  }

  // Fallback Text-to-Speech s·ª≠ d·ª•ng Web Speech API
  fallbackTextToSpeech(text) {
    return new Promise((resolve, reject) => {
      if (!("speechSynthesis" in window)) {
        reject(new Error("Tr√¨nh duy·ªát kh√¥ng h·ªó tr·ª£ Speech Synthesis"));
        return;
      }

      const utterance = new SpeechSynthesisUtterance(text);
      utterance.lang = "vi-VN";
      utterance.rate = 1.0;
      utterance.pitch = 1.0;

      utterance.onend = () => {
        resolve(true);
      };

      utterance.onerror = (error) => {
        reject(error);
      };

      speechSynthesis.speak(utterance);
    });
  }

  // Ph√°t audio t·ª´ blob
  playAudio(audioBlob) {
    const audioUrl = URL.createObjectURL(audioBlob);
    const audio = new Audio(audioUrl);

    audio.onended = () => {
      URL.revokeObjectURL(audioUrl);
    };

    return audio.play();
  }

  // Ki·ªÉm tra tr·∫°ng th√°i ghi √¢m
  getRecordingStatus() {
    return this.isRecording;
  }
}

export default new AgoraService();
